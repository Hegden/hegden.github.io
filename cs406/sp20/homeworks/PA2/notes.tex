\documentclass{article}
\usepackage[margin=1in]{geometry}
\usepackage{tabulary}
\usepackage{float}
\usepackage{lipsum}
\usepackage[utf8]{inputenc}
\usepackage{hyperref}
\usepackage{graphicx}
\usepackage{graphics}
\usepackage{listings}
\lstset
{ %Formatting for code in appendix
    language=C,
    basicstyle=\small,
    numbers=left,
    stepnumber=1,
    showstringspaces=false,
    tabsize=1,
    breaklines=true,
    breakatwhitespace=false,
}
\usepackage{xcolor}
\lstset{escapeinside={<@}{@>}}
\usepackage[T1]{fontenc}
\usepackage[parfill]{parskip}
\newcommand{\code}[1]{\textsf{#1}}
\setlength{\parindent}{0em}
\begin{document}
\begin{center}{\LARGE CS406: Compilers} \end{center}
\begin{center}{\large Programming Assignment 2: Parser,  Due: 13/2/2020} \end{center}

\bigskip



\section{Introduction}
Your goal in this step is to generate the parser for your programming language's grammar. By the end of this step your compiler should be able to take a source file as the input and parse the content of  that file returning "Accepted" if the file's content is correct according to the grammar or "Not Accepted" if it is not. \\

Now the scanner created in the first step will be modified to feed the parser.  Instead of printing the tokens, the scanner has to return what token is recognized in each step. 

\section{Background}
The job of a parser is to convert a stream of tokens (as identified by the scanner) into a {\em parse tree}: a representation of the structure of the program. So, for example, a parser will convert:

\texttt{A := B + 4}

into a tree that looks something like:

\begin{figure}[H]
	\includegraphics[scale=0.5]{parsetree}
\end{figure}

This tree may look confusing, but it fundamentally captures the structure of an {\em assignment statement}: an assignment statement is an {\em assignment expression} followed by a semicolon. An assignment expression is decomposed into an {\em identifier} followed by an assignment operation followed by an {\em expression}. That expression is decomposed into a bunch of {\em primary} terms that are combined with addition and subtraction, and those primary terms are decomposed into a bunch of {\em factors} that are combined with multiplication and division (this weird decomposition of expressions captures the necessary order of operations). Eventually, those {\em factors} become identifiers or constants.

One important thing to note is that the {\em leaves} of the tree are the tokens of of the program. If you read the leaves of the tree left to right (ignoring lambdas, since just represent the empty string), you get:

\texttt{IDENTIFIER ASSIGN\_OP IDENTIFIER PLUS\_OP INTLITERAL}

Which is exactly the tokenization of the input program!

\subsection{Context-free grammars}
To figure out how each construct in a program (an expression, an if statement, etc.) is decomposed into smaller pieces and, ultimately, tokens, we use a set of rules called a {\em context-free grammar}. 
These rules tell us how constructs (which we call ``non-terminals'') can be decomposed and written in terms of other constructs and tokens (which we call ``terminals''). 

\subsection{Micro}
The context-free grammar that defines the structure of Micro (the name of your programming language) is:

\begin{lstlisting}[numbers=none]
/* Program */
program           -> PROGRAM id BEGIN pgm_body END 
id                -> IDENTIFIER
pgm_body          -> decl func_declarations
decl                -> string_decl decl | var_decl decl | empty

/* Global String Declaration */
string_decl       -> STRING id := str ;
str               -> STRINGLITERAL

/* Variable Declaration */
var_decl          -> var_type id_list ;
var_type            -> FLOAT | INT
any_type          -> var_type | VOID 
id_list           -> id id_tail
id_tail           -> , id id_tail | empty

/* Function Paramater List */
param_decl_list   -> param_decl param_decl_tail | empty
param_decl        -> var_type id
param_decl_tail   -> , param_decl param_decl_tail | empty

/* Function Declarations */
func_declarations -> func_decl func_declarations | empty
func_decl         -> FUNCTION any_type id (param_decl_list) BEGIN func_body END
func_body         -> decl stmt_list 

/* Statement List */
stmt_list         -> stmt stmt_list | empty
stmt              -> base_stmt | if_stmt | for_stmt
base_stmt         -> assign_stmt | read_stmt | write_stmt | return_stmt

/* Basic Statements */
assign_stmt       -> assign_expr ;
assign_expr       -> id := expr
read_stmt         -> READ ( id_list );
write_stmt        -> WRITE ( id_list );
return_stmt       -> RETURN expr ;

/* Expressions */
expr              -> expr_prefix factor
expr_prefix       -> expr_prefix factor addop | empty
factor            -> factor_prefix postfix_expr
factor_prefix     -> factor_prefix postfix_expr mulop | empty
postfix_expr      -> primary | call_expr
call_expr         -> id ( expr_list )
expr_list         -> expr expr_list_tail | empty
expr_list_tail    -> , expr expr_list_tail | empty
primary           -> ( expr ) | id | INTLITERAL | FLOATLITERAL
addop             -> + | -
mulop             -> * | /

/* Complex Statements and Condition */ 
if_stmt           -> IF ( cond ) decl stmt_list else_part FI
else_part         -> ELSE decl stmt_list | empty
cond              -> expr compop expr
compop            -> < | > | = | != | <= | >=

init_stmt         -> assign_expr | empty
incr_stmt         -> assign_expr | empty

for_stmt          -> FOR ( init_stmt ; cond ; incr_stmt ) decl stmt_list ROF
\end{lstlisting}

So this grammar tells us, for example, that an \texttt{if\_stmt} looks like the keyword \texttt{IF} followed by an open parenthesis, followed by a \texttt{cond} expression followed by some \texttt{decl} (declarations) followed by a \texttt{stmt\_list} followed by an \texttt{else\_part} followed by the keyword \texttt{FI}.

An input program matches the grammar (we say ``is accepted by'' the grammar) if you can use the rules of the grammar (starting from \texttt{program}) to generate the set of tokens that are in the input file. If there is no way to use the rules to generate the input file, then the program does not match the grammar, and hence is not a syntactically valid program.

\section{Building a Parser}
There are many tools that make it relatively easy to build a parser for a context free grammar (in class, we will talk about how these tools work): all you need to do is provide the context-free grammar and some actions to take when various constructs are recognized. The tools we recommend using are: 
\begin{itemize}
	\item	bison (this is a tool that is meant to work with scanners built using flex). Note that integrating a flex scanner with bison requires a little bit of work. The process works in several steps that seem interlocking: 
		\begin{enumerate}
			\item Define your token names as well as your grammar in your bison input file (called something like \texttt{microParser.y})
			\item Run \texttt{bison -d -o microParser.cpp} on \texttt{microParser.y}, it will create two output files: \texttt{microParser.cpp} (which is your parser) and \texttt{microParser.tab.hpp} (which is a header file that defines the token names)
			\item In your scanner file (called something like \texttt{microLexer.l}), add actions to each of your token regexes to simply return the token name (from your \texttt{.y} file) you defined. ({\em Warning}: make sure that you don't have a token named \texttt{BEGIN} even if the regex matches the string ``BEGIN'', because that will cause weird, hard-to-find errors. Call that token something like \texttt{\_BEGIN}.) To make sure that your scanner compiles, you will need to put \texttt{\#include ``microParser.tab.hpp''} in the part of your \texttt{.l} file where you can include C code.
			\item Run flex on \texttt{microLexer.l} to produce \texttt{lex.yy.cpp}
			\item In another file, write a \texttt{main} function (this file will also need to include \texttt{microParser.tab.hpp}). Your main function should open the input file and store the file handle in a variable called \texttt{yyin}. Calling \texttt{yyparse()} will then run your parser on the file associated with \texttt{yyin}.
			\item Compile together all of \texttt{microParser.cpp}, \texttt{lex.yy.cpp}, and your \texttt{main} function to build your compiler.
		\end{enumerate}

	\item ANTLR (this is the same tool that can also build lexers). You should define your grammar in the same \texttt{.g4} file in which you defined your lexer. 
		\begin{enumerate}
			\item Running ANTLR on that \texttt{.g4} file will produce both a Lexer class and a Parser class. 
			\item In your main file, rather than initializing a lexer and then grabbing tokens from it (as you may have done in step 1), you instead initialize a lexer, initialize a \texttt{CommonTokenStream} from that lexer, then initialize a parser with the \texttt{CommonTokenStream} you just created. 
			\item You can then call a function with the same name as your top-level construct (probably program) on that parser to parse your input.
		\end{enumerate}
\end{itemize}

\section{What you need to do}
The grammar for Micro is given above. All you need to do is have your parser parse the given input file and print \texttt{Accepted} if the input file correctly matches the grammar, and \texttt{Not Accepted} if it doesn't (i.e., the input file cannot be produced using the grammar rules).

In bison, you can define a function called \texttt{yyerror} (look at the documentation for the appropriate signature) that is called if the parser encounters an error. 

In antlr, this is a little more complicated. You will need to create a new ``error strategy'' class (extend \texttt{DefaultErrorStrategy}) that overrides the function \texttt{reportError}. You can then set this as the error handler for your parser by calling \texttt{setErrorHandler} on your parser before starting to parse.

\paragraph{Sample inputs and outputs:} \href{https://hegden.github.io/cs406/homeworks/PA2/inputs.zip}{inputs} and \href{https://hegden.github.io/cs406/homeworks/PA2/outputs.zip}{outputs}.

\section{What you need to submit}
\begin{itemize}
	\item All of the necessary code for your compiler that you wrote yourself. You do not need to include the ANTLR jar files if you are using ANTLR.

	\item A Makefile with the following targets:
		\begin{enumerate}
			\item \texttt{compiler}: this target will build your compiler
			\item \texttt{clean}: this target will remove any intermediate files that were created to build the compiler
			\item \texttt{team}: this target will print the same team information that you printed in step 0.
		\end{enumerate}
	\item A shell script (this must be written in bash) called \texttt{runme} that runs your scanner. This script should take in two arguments: first, the input file to the scanner and second, the filename where you want to put the scanner's output. You can assume that we will have run \texttt{make compiler} before running this script.
\end{itemize}

While you may create as many other directories as you would like to organize your code or any intermediate products of the compilation process, both your \texttt{Makefile} and your \texttt{runme} script should be in the root directory of your repository.

{\em Do not submit any binaries}. Your git repo should only contain source files; no products of compilation.

You should tag your programming assignment submission as \textsf{submission}
\end{document}
